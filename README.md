# Yandex_ML_2
две рейтинговые задачи из тренировок по мл 2.0 
two rating tasks for ml trainings 2.0 (NLP)
## Проект 1

### Задание

В 2084 году человечество установило первый контакт с внеземной цивилизацией, обитающей на планете Зета в созвездии Андромеды. Инопланетяне, которых назвали зетанами, обладают высокоразвитой технологией и стремятся к обмену знаниями с землянами. Для успешного установления контакта и развития взаимовыгодных отношений, чрезвычайно важно наладить эффективную коммуникацию.

Зетаны предоставили человечеству обширную текстовую библиотеку на своем языке, включающую как оригинальные произведения, так и переводы известных земных текстов. В свою очередь, они получили доступ к библиотекам Земли. Однако алгоритмы машинного перевода пока плохо справляются с необычным строением языка зетан, что делает перевод неточным и неполным.

Необходимо обучить модель перевода с языка зетан на английский.

sentence-wise BLEU score

### Реализация

**Общая идея**

Как видно по данным, структуры языка примерно соответствуют. Можно попробовать применить seq2seq модель для перевода

**Предобработка данных**

Данные необходимо почистить:

1. Удалить те предложения, где оригинал сильно длиннее перевода
2. Удалить те, у которых источник или перевод пустой
3. Удалить те, где не совпадают спецсимволы и знаки пунктуации(? несколько раз)
4. Удалить те, где не совпадают цифры

**Выводы по данным**

Из 300 000 такой очисткой можно получить 250 000

**Архитектуры**

Либо использовать RNN-LSTM модели, либо Трансфомер
Была выбрана transformer-base модель со следующими параметрами:
- эмбеддинг 196
- 6 голов внимания
- в каждой голове по 3 энкодер и 3 декодер слоя
- полносвязный слой 512

Количество параметров модели 21 млн.
70 эпох обучения.

В качестве токенов были выбраны слова, предложения почищены от спецсимволов и знаков пунктуации. 
В качестве инициализации эмбеддинг слоев были выбраны векторы из матриц весов двух обученные word2vec моделей,
предполагалось, что это поможет модели лучше понять структуру языков. Были взяты все слова с частотой не менее 3.
В итоге, в английском 54 тыс. слов, в зетан = 23 тыс.

**Метрики**
Кроссэнтропия

**Результаты**

BLEU 1.81

Обучение сильно скатывалось в предсказание <unk>, поэтому было применено небольшое ухищрение - при генерации добавлен софтмакс с температурой 0.3.

По результатам разбора сделаны такие выводы:
- Лучше использовать BPE токенизацию
- Лучше делать больше слоев энкодера, поскольку он отвечает за основное преобразование информации.

==================================

## Проект 2

### Задание


Вам нужно натренировать модель для задачи детекции звуковых ивентов, 
прогнать полученную модель на тестовых данных и загрузить полученный результат в формате tsv в яндекс контест.
30 классов.

### Реализация

**Общая идея**

Получаем из музыкальных фрагментов мел-спектрограммы, с которыми работаем, как с изображениями.
Задачу сводим к задаче классификации изображений. Либо CNN, либо ViT

**Предобработка данных**
Переводим звук в мел-спектрограмму.
Данных немного, поэтому добавляем аугментацию в виде равновероятного выбора из следущих действий для спектрограммы:

- Случайное маскирование от 2 до 5 небольших областей спектрограммы
- Случайный кусок спектрограммы меньшей величины
- Добавление нормального шума
- Добавление шума из списка шумов
- Ничего не делать

**Выводы по данным**

Классы несбалансированы

**Архитектуры**

Попробовал ViT16 и ResNet-18, vit слишком тяжелый для такой задачи, резнет лучше обобщает.
В итоге резнет с двумя базовыми блоками вместо четырёх показал себя лучше всего.
678110 обучаемых параметров.

Из-за наличия триплет лосса на вход подаем якорное, негативное и позитивное изображение.

60 эпох, оптимайзер Адам, расписание темпа сходимости - Полиномиальное либо Уменьшение в 2 раза на плато из 2х эпох.

**Метрики**

Кроссэнтропия со взвешанными классами + триплет лосс с отступом 1.0 (пробовал 0.3, 0.7, 1.2)

**Результаты**

Accuracy = 53.2, после чего ни смотря ни на какие ухищрения переобучается. 



