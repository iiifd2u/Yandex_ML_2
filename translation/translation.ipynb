{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4082,
     "status": "ok",
     "timestamp": 1732052590635,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "1kA-9xgAUrLs",
    "outputId": "1244c3c9-5676-4662-a97f-4f0eebb35e6c"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14663,
     "status": "ok",
     "timestamp": 1732052606264,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "fNwSZkPkU8hC",
    "outputId": "015d7c9c-c3b5-4bf0-dc4e-d59258597d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkF1nZHHU_JL"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from collections import Counter\n",
    "from configparser import ConfigParser\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "import transformers\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm.auto import tqdm, trange\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1732052626693,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "h--lvo6gVFhy",
    "outputId": "338dbe44-4659-40ff-a8af-34e15a232131"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_I8Rg24VMNH"
   },
   "outputs": [],
   "source": [
    "root = \"/content/drive/MyDrive/nlp_train_contests/1\"\n",
    "models_dir = os.path.join(root, \"models\")\n",
    "data_dir = os.path.join(root, \"data\")\n",
    "submission_dir = os.path.join(root, \"submissions\")\n",
    "config_dir = os.path.join(root, \"configs\")\n",
    "config_path = os.path.join(config_dir, \"conf.ini\")\n",
    "\n",
    "\n",
    "MIN_FREQ = 3 #Второй вариант для 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7obwEbHVMP8"
   },
   "outputs": [],
   "source": [
    "def get_date_id():\n",
    "    now = datetime.now()\n",
    "    date_id = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return date_id\n",
    "\n",
    "\n",
    "# def save_submission(submission:List, save_dir:str):\n",
    "#   os.makedirs(save_dir, exist_ok=True)\n",
    "#   filename = os.path.join(f\"submission_{get_date_id()}.txt\")\n",
    "#   with open(os.path.join(save_dir, filename), \"w\") as f:\n",
    "#       for line in submission:\n",
    "#           f.write(\"{}\\n\".format(\" \".join(list(map(str, map(int, line))))))\n",
    "\n",
    "def save_submission(submission:List[dict], save_dir:str):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = os.path.join(f\"submission_{get_date_id()}.jsonl\")\n",
    "    with open(os.path.join(save_dir, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "        for di in submission:\n",
    "            line = json.dumps(di, sort_keys=True, ensure_ascii=False, separators=(',', ':'))\n",
    "            f.write(line+\"\\n\")\n",
    "\n",
    "def save_models(model:nn.Module,optimizer: torch.optim.Adam, scheduler, model_name:str, save_dir:str):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = os.path.join(f\"{model_name}_{get_date_id()}.pt\")\n",
    "    try:\n",
    "        # torch.save(model.state_dict(), os.path.join(save_dir, filename))\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_step_dict': scheduler.state_dict()\n",
    "            }, os.path.join(save_dir, filename))\n",
    "    except Exception as e:\n",
    "        print(f\"ошибка в сохранении модели {e}\")\n",
    "\n",
    "def load_weights(model:nn.Module, path:str, device:torch.device):\n",
    "    if os.path.exists(path):\n",
    "        model.load_state_dict(torch.load(path, map_location=device, weights_only=True))\n",
    "        print(f\"Модель успешно загружена из {path}\")\n",
    "    else:\n",
    "        print(\"Путь модели не существует!\")\n",
    "\n",
    "\n",
    "def load_config(path: str) ->ConfigParser:\n",
    "    try:\n",
    "        config = ConfigParser()\n",
    "        config.read(path)\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        print(f\"ошибка чтения конфига {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1732052626694,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "0Sn6wItXVMSx",
    "outputId": "0222e779-f1ff-44e7-9e79-162606dd981f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/nlp_train_contests/1/data/val'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = os.path.join(data_dir, \"train\")\n",
    "val_path = os.path.join(data_dir, \"val\")\n",
    "test_path = os.path.join(data_dir, \"test_no_reference\")\n",
    "val_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OnhbdqCVMVW"
   },
   "outputs": [],
   "source": [
    "val_data = []\n",
    "with open(val_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        temp_dict = json.loads(line)\n",
    "        val_data.append([temp_dict[\"src\"], temp_dict[\"dst\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GythbeoVMYB"
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        temp_dict = json.loads(line)\n",
    "        train_data.append([temp_dict[\"src\"], temp_dict[\"dst\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1732052630161,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "-UdgV0riVWFn",
    "outputId": "ab0a3ba9-9a07-4903-d47f-d2d60973ed45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина трейна = 300000\n",
      "Длина вала = 500\n"
     ]
    }
   ],
   "source": [
    "##### Формат: {\"dst\":\"....\", \"src\":....}\n",
    "print(f\"Длина трейна = {len(train_data)}\")\n",
    "print(f\"Длина вала = {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoNpyxX5VWIU"
   },
   "outputs": [],
   "source": [
    "for line in val_data:\n",
    "    line[0] = line[0].replace(\"▵\", \" ▵\")\n",
    "\n",
    "for line in train_data:\n",
    "    line[0] = line[0].replace(\"▵\", \" ▵\")\n",
    "\n",
    "val_data = list(map(lambda el:(word_tokenize(el[0]), word_tokenize(el[1])), val_data))\n",
    "train_data = list(map(lambda el:(word_tokenize(el[0]), word_tokenize(el[1])), train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ssLSS2IVWLM"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EMB_SIZE = ENC_EMB_DIM = DEC_EMB_DIM = 192 #Второй вариант для 192, на компе щас для 96\n",
    "NHEAD = 6\n",
    "FFN_HID_DIM = 512 #Второй вариант для 192, на компе щас для 256\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "DEVICE = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 70\n",
    "\n",
    "SRC_LANGUAGE = 'ze'\n",
    "TGT_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwkf1gTvVWNs"
   },
   "outputs": [],
   "source": [
    "def train_wv_src():\n",
    "    train_data_src = [[\"<sos>\"] +el[0]+[\"<eos>\"] for el in train_data]\n",
    "    model_src = Word2Vec(sentences=train_data_src, vector_size=ENC_EMB_DIM, window=5, min_count=MIN_FREQ, workers=4)\n",
    "    model_src.save(os.path.join(models_dir, f\"model_src_{EMB_SIZE}\"))\n",
    "\n",
    "    # Store just the words + their trained embeddings.\n",
    "    word_vectors = model_src.wv\n",
    "\n",
    "    np.random.seed(1234)\n",
    "    unk_arr = np.random.random(size = ENC_EMB_DIM).astype(np.float32)\n",
    "    unk_arr /= np.linalg.norm(unk_arr).astype(np.float32)\n",
    "    pad_arr = np.random.random(size = ENC_EMB_DIM).astype(np.float32)\n",
    "    pad_arr /= np.linalg.norm(pad_arr).astype(np.float32)\n",
    "\n",
    "    additional = {\"<unk>\": unk_arr, \"<pad>\":pad_arr}\n",
    "\n",
    "    for k, v in additional.items():\n",
    "        word_vectors.add_vector(k, v)\n",
    "\n",
    "    word_vectors.save(os.path.join(models_dir, f\"model_src_vocab_{EMB_SIZE}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vj87RtnXVWQV"
   },
   "outputs": [],
   "source": [
    "def train_wv_trg():\n",
    "    train_data_trg = [[\"<sos>\"] +el[1]+[\"<eos>\"]  for el in train_data]\n",
    "    model_trg = Word2Vec(sentences=train_data_trg, vector_size=DEC_EMB_DIM, window=5, min_count=MIN_FREQ, workers=4)\n",
    "    model_trg.save(os.path.join(models_dir, f\"model_trg_{EMB_SIZE}\"))\n",
    "\n",
    "    # Store just the words + their trained embeddings.\n",
    "    word_vectors = model_trg.wv\n",
    "\n",
    "    unk_arr = np.random.random(size = DEC_EMB_DIM).astype(np.float32)\n",
    "    unk_arr /= np.linalg.norm(unk_arr).astype(np.float32)\n",
    "    pad_arr = np.random.random(size = DEC_EMB_DIM).astype(np.float32)\n",
    "    pad_arr /= np.linalg.norm(pad_arr).astype(np.float32)\n",
    "\n",
    "    additional = {\"<unk>\": unk_arr, \"<pad>\":pad_arr}\n",
    "\n",
    "    for k, v in additional.items():\n",
    "        word_vectors.add_vector(k, v)\n",
    "\n",
    "    word_vectors.save(os.path.join(models_dir, f\"model_trg_vocab_{EMB_SIZE}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64972,
     "status": "ok",
     "timestamp": 1731942290036,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "nFzNEWDzVWTC",
    "outputId": "a4c5c3a7-732a-4351-dd54-11a4c4daa618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train_wv_src()\n",
    "# train_wv_trg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5122,
     "status": "ok",
     "timestamp": 1732052704500,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "v6bkX4YJWTpP",
    "outputId": "7610e34a-983a-49e0-b47d-a25ba2829118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 54069, out = 23161\n"
     ]
    }
   ],
   "source": [
    "wv_src= KeyedVectors.load(os.path.join(models_dir, f\"model_src_vocab_{EMB_SIZE}\"), mmap='r')\n",
    "SRC_VOCAB_SIZE  = len(wv_src)\n",
    "wv_trg = KeyedVectors.load(os.path.join(models_dir, f\"model_trg_vocab_{EMB_SIZE}\"), mmap='r')\n",
    "TGT_VOCAB_SIZE = len(wv_trg) #\"Размеры словаря и word2vec должны совпадать!\"\n",
    "print(f\"input = {SRC_VOCAB_SIZE }, out = {TGT_VOCAB_SIZE }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8pHpW1vWTsJ"
   },
   "outputs": [],
   "source": [
    "trg_unk_idx = wv_trg.key_to_index[\"<unk>\"]\n",
    "trg_pad_idx = wv_trg.key_to_index[\"<pad>\"]\n",
    "src_unk_idx = wv_src.key_to_index[\"<unk>\"]\n",
    "src_pad_idx =wv_src.key_to_index[\"<pad>\"]\n",
    "\n",
    "trg_sos_idx = wv_trg.key_to_index[\"<sos>\"]\n",
    "trg_eos_idx = wv_trg.key_to_index[\"<eos>\"]\n",
    "src_sos_idx = wv_src.key_to_index[\"<sos>\"]\n",
    "src_eos_idx =wv_src.key_to_index[\"<eos>\"]\n",
    "\n",
    "sos_token, eos_token, pad_token = \"<sos>\", \"<eos>\", \"<pad>\"\n",
    "unk_token = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJKU9rf79uAx"
   },
   "outputs": [],
   "source": [
    "def encode_W2V(sent, wv, default_idx:int):\n",
    "    tokenized = [\"<sos>\"] + sent + [\"<eos>\"]\n",
    "    return [wv.get_index(tok, default=default_idx) for tok in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732052704501,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "eIZc1hAqWTu9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a104c000-0c45-4661-fbc2-cb0594d74bf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 25]) torch.Size([256, 41])\n"
     ]
    }
   ],
   "source": [
    "def collate_batch(batch):\n",
    "    src_list, trg_list = [], []\n",
    "    for src, trg in batch:\n",
    "        src_encoded = encode_W2V(src, wv=wv_src, default_idx=src_unk_idx)[::-1]\n",
    "        src_list.append(torch.tensor(src_encoded))\n",
    "\n",
    "        trg_encoded = encode_W2V(trg, wv=wv_trg, default_idx=trg_unk_idx)\n",
    "        trg_list.append(torch.tensor(trg_encoded))\n",
    "\n",
    "    src_padded = pad_sequence(src_list, padding_value=wv_src.key_to_index[pad_token], batch_first=True)\n",
    "    trg_padded = pad_sequence(trg_list, padding_value=wv_trg.key_to_index[pad_token], batch_first=True)\n",
    "\n",
    "    return src_padded, trg_padded\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_data, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "src_batch, trg_batch = next(iter(train_dataloader))\n",
    "print(src_batch.shape, trg_batch.shape)\n",
    "val_dataloader = DataLoader(val_data, BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6YEAOnLWTxX"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "    src_padding_mask = (src == src_pad_idx)\n",
    "    tgt_padding_mask = (tgt == trg_pad_idx)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtSs-b2TWT0N"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"\n",
    "        :param max_len: Input length sequence.\n",
    "        :param d_model: Embedding dimension.\n",
    "        :param dropout: Dropout value (default=0.1)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs of forward function\n",
    "        :param x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int,\n",
    "        num_decoder_layers: int,\n",
    "        emb_size: int,\n",
    "        nhead: int,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1508,
     "status": "ok",
     "timestamp": 1732054521693,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "ZUo-fO8JWT2v",
    "outputId": "77c33204-8c31-4d9f-e459-254dcf1fd72a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21,822,649 total parameters.\n",
      "21,822,649 training parameters.\n",
      "Seq2SeqTransformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=192, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=192, bias=True)\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=192, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=192, bias=True)\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=192, out_features=23161, bias=True)\n",
      "  (src_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(54069, 192)\n",
      "  )\n",
      "  (tgt_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(23161, 192)\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Seq2SeqTransformer(\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS,\n",
    "    EMB_SIZE,\n",
    "    NHEAD,\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    FFN_HID_DIM\n",
    ").to(DEVICE)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=wv_trg.key_to_index[pad_token])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "# scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=40, num_training_steps=100*300_000//30_000)\n",
    "# после можно поменять на\n",
    "scheduler =torch.optim.lr_scheduler.PolynomialLR(optimizer, 50*300_000/30_000) #lr меняем на последний полученный\n",
    "\n",
    "# save_models(model, optimizer, scheduler, model_name=\"combo_512\", save_dir=models_dir)\n",
    "\n",
    "\n",
    "#Загрузить модель и состояние оптимизатора\n",
    "#Сразу сохранить и проверить что работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk_kF1G5WT5p"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, scheduler):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    scheduler_stepper=0\n",
    "    scheduler_iterator = 0\n",
    "    for src, tgt in tqdm(train_dataloader, total=len(list(train_dataloader))):\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        scheduler_stepper+=src.shape[0]\n",
    "\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        logits = model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "        if scheduler_stepper>=30_000:\n",
    "             scheduler.step()\n",
    "             scheduler_stepper=0\n",
    "             scheduler_iterator+=1\n",
    "             print(f\"lr = {scheduler.get_lr()}, loss = {losses/(30_000*scheduler_iterator)}\")\n",
    "\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    print('Validating')\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for src, tgt in tqdm(val_dataloader, total=len(list(val_dataloader))):\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask\n",
    "        )\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666,
     "referenced_widgets": [
      "03e572548b614b33a7e2867400e37400",
      "2766f4e8dd8b447d9d6ea0bbabc42c3b",
      "719f768998624922b99c92af38071c63",
      "e4bea96095194e8b8c91c59094be2be9",
      "cb9b9306ead74461a98868ed0a3a166b",
      "0df9d2e9ec2940e6bf2a9e9de3578c3c",
      "896097451c7f4e4088e4ce19267e4bf5",
      "b0b8ccd6b934490dba720127948a4b9e",
      "f6606893463043eca4e5da2277d3259a",
      "959f4b00b7fb4ff58b612332143e193c",
      "540bd0c1c80f43fea52aac803a229441"
     ]
    },
    "executionInfo": {
     "elapsed": 179430,
     "status": "error",
     "timestamp": 1732055106389,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "XKwyFGPrWT7s",
    "outputId": "64625754-790e-4d7b-8e47-f7de7ed24bde"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(models_dir, \"combo_512_20241119_161553.pt\"), weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "scheduler.load_state_dict(checkpoint[\"scheduler_step_dict\"])\n",
    "\n",
    "train_loss_list, valid_loss_list = [], []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, optimizer, scheduler)\n",
    "    valid_loss = evaluate(model)\n",
    "    end_time = time.time()\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    if epoch%3==0:\n",
    "        save_models(model, optimizer, scheduler, model_name=\"combo_512\", save_dir=models_dir)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {valid_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s \\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKCG4HZTXJFv"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def save_plots(train_loss, valid_loss):\n",
    "    \"\"\"\n",
    "    Function to save the loss plots to disk.\n",
    "    \"\"\"\n",
    "    # Loss plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss, color='blue', linestyle='-',\n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        valid_loss, color='red', linestyle='-',\n",
    "        label='validataion loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join('outputs', 'loss.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4nn334VXT_q"
   },
   "outputs": [],
   "source": [
    "save_plots(train_loss_list, valid_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hjny9gtBZ_dE"
   },
   "outputs": [],
   "source": [
    "# Helper function to generate output sequence using greedy algorithm.\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol, temperature:float = 0.3):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        if i == 0:\n",
    "            ys = ys.transpose(1, 0)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(1))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out\n",
    "\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        #p_next = F.softmax(prob.squeeze(0) / temperature, dim=-1).detach().cpu().data.numpy()\n",
    "        #top_index = np.random.choice(len(wv_trg), p=p_next)\n",
    "        # # next_word = torch.tensor([top_index], dtype=torch.long)\n",
    "        #next_word = top_index\n",
    "\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        if next_word == trg_eos_idx:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX0HAgnWZ_fr"
   },
   "outputs": [],
   "source": [
    "def translate(model: torch.nn.Module, src_sentence: list):\n",
    "    model.eval()\n",
    "    src = torch.tensor(encode_W2V(src_sentence, wv=wv_src, default_idx=src_unk_idx)).view(1, -1).to(device)\n",
    "    num_tokens = src.shape[1]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=trg_sos_idx).flatten()\n",
    "    return \" \".join([wv_trg.index_to_key[tok] for tok in tgt_tokens.cpu().numpy()]).replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5291,
     "status": "ok",
     "timestamp": 1731911634453,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "DrpNqDDDZ_iJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "91e5fba7-bfd7-4e9e-b82e-a157624d2859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: ['◝▴▱▱◠▫◂◓', \"206'◳◬\", '◗▢▱▴◎◪▨', '◗◉◫▦', '◀◭▫▩▦', '◕◪▼◪', '◠▽◠▨▫◠', '▨◠▱◈▪◐▪▦▪▢◬', '◚▴', '◠◞▪▱', '▴▫▨◗▦▱◗◐◗', '◗▢▱◪◎▴', '◍◬◓◞◠▫◬▦▪▦', '◚▴◓◗▱◎◪◈◫◐◗▦◫', '◈◭◒▩▦◭▦', '▵']\n",
      "GT: ['Imagine', 'you', 'stayed', 'up', 'all', 'night', 'to', 'watch', 'the', 'full', 'Bellator', '206', ',', 'but', 'you', 'did', \"n't\", 'get', 'to', 'see', 'the', 'main', 'event', '.']\n",
      "PRED:  I 'm not going to be a <unk> , and I 'm not going to be a <unk> , and I 'm\n",
      "\n",
      "SRC: ['◄◠▨◪◈◂▦▽◠', '◝◠◒◀◠▨◠▦▪', '◁◧◓◠▦', '◁◠▴◚■', '◪◒◗', '◁◧◓◫▼◠', '◚▴', '◧◐▱◨', '◙◨◞▷▨◂■', '◄◠▨◪◈◂▦◳◠', '◓◪◍▴◓◠▦◈▾◎◨▦◈◠', '◭▱▨◪▦◫▦', '◠◈◬▦▪', '◈▴◐◫◒▫◫◓◪◓◪▨', '▯◦▶◱', '◚◪', '○◚◓▾▻◠', \"◝◗◓▱◗◐◫'▦▴\", '▨◠▫▪▱▪◎▪▦', '◣▦◭▦◭', '◠◉◎◠', '◳◣▦▩▦◈◪▨◫', '◂◳▱◠◓◬▦◬', '30', '◰◳▱▩▱', \"2018'◈▴\", '▮▫◓◨◎◗▼◠■', \"◄◠▨▴◈◧▦◳◠'◈◠\", '▨▾▱▱◠▦◈◬▱◠◓', '▵']\n",
      "GT: ['Macedonian', 'Prime', 'Minister', 'Zoran', 'Zaev', ',', 'his', 'wife', 'Zorica', ',', 'and', 'his', 'son', 'Dusko', 'voted', 'for', 'the', 'Macedonian', 'referendum', 'on', 'changing', 'the', 'country', \"'s\", 'name', ',', 'which', 'will', 'pave', 'the', 'way', 'for', 'its', 'accession', 'to', 'NATO', 'and', 'the', 'European', 'Union', 'in', 'Strumica', ',', 'Macedonia', ',', 'on', 'September', '30', ',', '2018', '.']\n",
      "PRED:  <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> ,\n",
      "\n",
      "SRC: ['◲▨◫', '▽▪▱◈◠', '◀◗◓', '◈▩▢◪▦▱▴▦◪▦', '◪▫▨◫▦▱◗◐▴', '12', '▵', '▨▴▢', '▨◠▫◬▱◠◓◠▨', '◓◪▨◂◓', '▨▪◓◠▦', '◄◗▼▨▴▱◞◧▦■', '▼◨◎◠◓▫▴◞◗', '◕◪◓◉▴▨▱◪◒▫◫◓◗▱◪▦', '◍◂◨◓◀◠▱▱', '◚◪', '◈◣◓▫▱◭', '(', '◍◧◨◓◞◂◎▴', ')', '◂▽▾▦▱◠◓▪▦◈◠', '◢◠▻▫◠▦', '◡◗◎', '◌◨◓◳▨', '▫◠◓◠◍▪▦◈◠▦', '◂▽◨▦◈◠▦', '◠▱◬▦◈▪', '▵']\n",
      "GT: ['Mickelson', ',', 'who', 'played', 'in', 'the', 'Biennale', 'for', 'a', 'record', '12th', 'time', ',', 'was', 'sent', 'to', 'the', 'bench', 'by', 'captain', 'Jim', 'Furyk', 'on', 'Saturday', 'for', 'four', 'bogeys', 'and', 'four', 'birdies', '.']\n",
      "PRED:  I 'm not a <unk> , and I 'm not going to be a <unk> , and I 'm not going to be a <unk> , and I 'm not going to be a\n",
      "\n",
      "SRC: ['◝◗◓', '▻◂▱◗◞', '◎▴◎▾◓▾▦▾▦', '◀◠◒▨◠', '◀◗◓', '◠◈◠◎◠', '◀▩▫◭▦', '◀◣▱◕▴▦◫▦', '◠◓▫◬▨', '◞◨◉', '◎◠▷◠▱▱◗', '◞◠▽▪▱◈▪◐▪▦◬', '◞◣◳▱▴◈◫◐◗', '◈◨▽◨▱◈◨', '▵']\n",
      "GT: ['One', 'of', 'the', 'police', 'officers', 'told', 'the', 'tenant', 'that', 'the', 'entire', 'surrounding', 'area', 'was', 'considered', 'a', 'crime', 'scene', '.']\n",
      "PRED:  I 'm not a <unk> , but I 'm not a <unk> , I 'm not a <unk> . \n",
      "\n",
      "SRC: ['▯◧◓◎◠▱◈◪', '▽◠▻▫▪◐▪◎▪▢', '◕◫◀◫', '◠▱▫◬', '▽▴◓◫▦▴', '◀◪◒', '◈◣◚◭◒', '▨◧◳◠▼◠◐▪▢', '▵', '◄◄◦', '▷◠▽◓◠▦▱◠◓▪▦◠', '◈◠▷◠', '◗◳◗◞◫▦◗', '◞◨▦◎◠▨', '◫◉◗▦', '◉◠▱▪◒◬◓▨◪▦', '◀◨', '◞◂◓◨▦▱◠', '▨◠◓◒▪▱◠◒▫▪▨', '▵']\n",
      "GT: ['We', \"'re\", 'going', 'to', 'reduce', 'the', 'number', 'of', 'fights', 'to', 'five', 'instead', 'of', 'the', 'usual', 'six', ',', 'and', 'we', \"'re\", 'trying', 'to', 'increase', 'it', 'for', 'the', 'fans', ',', 'but', 'it', \"'s\", 'too', 'much', '.']\n",
      "PRED:  I 'm not going to be a <unk> , but I 'm not going to be a <unk> , I 'm not going to be a\n",
      "\n",
      "SRC: ['▭◠◎◎◪▱', '◀◠◒▫◠', '◉◂▼◨◐▾▦', '◬◞▫◠▨◂▢', '◀▾▱◎◠▦◬▦', '▷◪◳▴▼◠▦◬◳▱◠', '◀◠◐◬◓◈◬◐▪▦◬', '◞◠▦◈◬◐◬▦▪', '◠▦▼◠▨', '◞◧▦◓◠◈◠▦■', '``', \"'◃◞▪◓◈◬\", '◀◪▦◗', '!', '◃◞◬◓◈◬', '◀▴▦◗', '!', \"'\", '◈◗◳▴', '◀◠◐▪◓◈◬◐◬▦▪', '◍◠◓▨', '◪▫▫◗◐◗▦◗', \"''\"]\n",
      "GT: ['According', 'to', 'Hummel', ',', 'at', 'first', 'he', 'thought', 'the', 'boy', 'was', 'excited', 'about', 'catching', 'the', 'lobster', ',', 'but', 'he', 'quickly', '``', 'realized', 'that', 'he', 'was', 'screaming', ',', \"'\", 'I', \"'ve\", 'been', 'bitten', '!', \"'\", \"''\"]\n",
      "PRED:  You know , I 'm not going to tell me , but I 'm not going to tell you , I 'm not going to tell you , I\n",
      "\n",
      "SRC: ['◝◄', '◆▴▦▴▱', '◢▾◓▾▱▾▦◠', '▨◧▦▾◒◠▦', '◙▪◒◫◒▱◪◓◫', '◝◠▨◠▦◬', '◑▴▱◗◈', '◘▱', '◄◨◠▱▱◫◎■', '▩▱▨◪◈◪▨◗', '◞◠◚◠◒', '◞▴▨◗▢◗▦▼◫', '▽▪▱▪▦◈◠', '◂▱◎◠◞◬▦◠', '◓◠◐◎◪▦', '▮◨◓◫◳◪▱◗', '◎◭▱▫◪▼◗▱▴◓▴', '◈▴', '◈◣▦◎▴▱▴◓◗', '◫◉◗▦', '◉◠◐◓◬◈◠', '◀◨▱◨▦◈◨', '▵']\n",
      "GT: ['Speaking', 'at', 'the', 'UN', 'General', 'Assembly', ',', 'Foreign', 'Minister', 'Walid', 'al-Moualem', 'also', 'called', 'on', 'Syrian', 'refugees', 'to', 'return', 'home', ',', 'despite', 'the', 'fact', 'that', 'the', 'country', 'has', 'been', 'at', 'war', 'for', 'eight', 'years', '.']\n",
      "PRED:  I 'm not going to be a <unk> , but I 'm not going to be a <unk> , I 'm not going to be a <unk> , I\n",
      "\n",
      "SRC: ['▶◪▼◠◚▩▢', '◗◈◈◗◠▱◠◓▪▦▪', '◓▴◈◈◪◈▴▦', \"▤◧▦◠▱◈◧'▦▾▦\", '◠◚◨▨◠▫▱◠◓◬', '○▱◎◠▦', '◈▴◓◕◫◞◫▦▴', '◈◠◚◠', '◠◉◎◠▽◠', '▷◠▢▪◓▱◠▦◬▽◧◓']\n",
      "GT: ['Ronaldo', 'has', 'denied', 'the', 'rape', 'allegations', ',', 'and', 'his', 'lawyers', 'are', 'planning', 'to', 'sue', 'the', 'German', 'magazine', '.']\n",
      "PRED:  I 'm not going to be a <unk> , but I 'm not going to be\n",
      "\n",
      "SRC: ['▬▷◠◈', '▭◠◎◎▴▱■', '▮◠▦', \"◙◗◪◕◧'◈◠\", \"◢▮▰◝►▶◑'◳▴\", '▼▾◎◠◓▫▴◞◫', '◞◠◀◠▷▪', '◠◓▨◠◈◠◒▱◠◓◬▽▱◠', '◈◠▱◬◒', '◳◠▻◎◠◳◠', '◀◠◒▱◠◈▪▨▫◠▦', '▽◠◓◬◎', '◞◠◠▫', '◞◧▦◓◠', '◉◧▼▾◐▾▦', '◀◠◐▪◓◠◓◠▨', '◳◠◓◈▪◎', '◫◞▫▴◈◗◐◫▦◫', '◈◨◳◈▾◐◨▦◨', '◚◪', '◉◂▼◨◐▾', '◞▾◈◠▦', '◉◬▨◠◓◎◠▨', '◗◉◗▦', '◀◗◓', '◕◓◨▻▱◠', '▨◠◳◬◐◠', '◀◗▦◪◓▴▨', '▽◠▦◬▦◠', '◕◫▫▫◗◐◫▦◫', '◞◇▽▱▴◈◗', '▵']\n",
      "GT: ['As', 'Chad', 'Hummel', 'told', 'KSWB-TV', '(', 'San', 'Diego', ')', 'in', 'an', 'interview', ',', 'he', 'was', 'diving', 'with', 'friends', 'on', 'Saturday', 'morning', 'when', 'he', 'heard', 'a', 'boy', 'screaming', 'for', 'help', 'about', 'half', 'an', 'hour', 'later', '.', 'He', 'and', 'his', 'friends', 'swam', 'to', 'the', 'scene', 'in', 'a', 'boat', 'and', 'pulled', 'the', 'boy', 'out', 'of', 'the', 'water', '.']\n",
      "PRED:  I 'm not a <unk> , and I 'm not going to be a <unk> , and I 'm not going to be a <unk> , and I 'm not going to be a <unk> , and I\n",
      "\n",
      "SRC: ['◅◗◍▫■', '2018', '▽▪▱◬▦▪▦', '▷◠▢◗◓◠▦', '◠◳◬▦◈◠', \"◊◞▨◂◉▽◠'◈◠\", \"▲▴◞▱◗▴'▦◫▦\", '◠◫▱▴', '▴◚◗▦◈◪', '▴◚▱◪▦◈◗', '▵']\n",
      "GT: ['The', 'couple', 'celebrated', 'their', 'wedding', 'in', 'June', '2018', 'at', 'Leslie', \"'s\", 'family', 'estate', 'in', 'Scotland', '.']\n",
      "PRED:  I 'm not going to be a <unk> , but I 'm not going to be a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# infer_sentences = [\n",
    "#     [\"Take a seat.\", \"Prends place !\"],\n",
    "#     [\"I'm not scared to die\", \"Je ne crains pas de mourir.\"],\n",
    "#     [\"You'd better make sure that it is true.\", \"Tu ferais bien de t'assurer que c'est vrai.\"],\n",
    "#     [\"The clock has stopped.\", \"L'horloge s'est arrêtée.\"],\n",
    "#     [\"Take any two cards you like.\", \"Prends deux cartes de ton choix.\"]\n",
    "# ]\n",
    "val_path = os.path.join(data_dir, \"val\")\n",
    "val_data = []\n",
    "with open(val_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        temp_dict = json.loads(line)\n",
    "        val_data.append([temp_dict[\"src\"], temp_dict[\"dst\"]])\n",
    "\n",
    "for line in val_data:\n",
    "    line[0] = line[0].replace(\"▵\", \" ▵\")\n",
    "\n",
    "\n",
    "val_data = list(map(lambda el:(word_tokenize(el[0]), word_tokenize(el[1])), val_data))\n",
    "\n",
    "for sentence in random.sample(val_data, 10):\n",
    "    print(f\"SRC: {sentence[0]}\")\n",
    "    print(f\"GT: {sentence[1]}\")\n",
    "    print(f\"PRED: {translate(model, sentence[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvYlaZMi_dy8"
   },
   "outputs": [],
   "source": [
    "test_path = os.path.join(data_dir, \"test_no_reference\")\n",
    "test_data = []\n",
    "with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        temp_dict = json.loads(line)\n",
    "        test_data.append(temp_dict[\"src\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFioFmQw_ZKT"
   },
   "outputs": [],
   "source": [
    "def make_submission_transformer(test_data:list[str])->list[dict]:\n",
    "    sub_list_dict = []\n",
    "    model.eval()\n",
    "    max_len = 50\n",
    "    with torch.no_grad():\n",
    "        pred_sents = []\n",
    "        # val_sents = []\n",
    "        for sentence in test_data:\n",
    "\n",
    "            src = sentence.replace(\"▵\", \" ▵\")\n",
    "            src = word_tokenize(src)\n",
    "\n",
    "            translation = translate(model, src)\n",
    "\n",
    "            sub_list_dict.append({\"dst\":translation, \"src\":sentence})\n",
    "        # references_list = [[ref] for ref in pred_sents]\n",
    "        # bleu_score_corpus = corpus_bleu(references_list, val_sents)\n",
    "        # print(\"Corpus BLEU Score: \", bleu_score_corpus)\n",
    "    return sub_list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yv6J99guB5y0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1581,
     "status": "ok",
     "timestamp": 1731862214431,
     "user": {
      "displayName": "Ilya Ya",
      "userId": "03342033783933917263"
     },
     "user_tz": -180
    },
    "id": "6UaeZt5iATal",
    "outputId": "d1227283-cbd2-43ea-aec1-ee13e94e07e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dst': \" I 'm not sure you need to tell that how you can get a , was the leave your husband in the\", 'src': '◲▦◠▦◬▦■ ◉◗▢◕◗ ◍◗▱◎ ▽◠▽▪▦◠ ◕▴◉◗▦▼▴ ◀◗◓◉◧▨ ◎▴◞◠▸ ◠▱◈▪▨ ◚◪ ◀◨ ◎◪◞◠▸▱◠◓◬▦ ◀◠▢▪▱◠◓▪ ▻◪▨ ◈◂◞▫◉◠ ◈▴◐◫▱◈◗▵'}\n",
      "{'dst': \" Let 's see if I 'll be gon na start with you , we can get off the <unk> , wrong And make it if we might be <unk> , then the you 'll put another one\", 'src': '▯▴▥ ◟◧◓▨▱◨ ◀◫◓ ◈◠◈◬■ ◉◂▼◨◐◨▦ ◠▦▦◪◞◗▦◗▦ ▽◠▢◈◬◐▪ ◚◪ ◳◠▦▱▪◒▱▪▨▱◠ ▨▴▦◈◗◞◗▦▴ ◕◣▦◈▴◓◈◗◐◫■ \"◀◫◓ ◞◫◳◠▷◗ ◈◠▷◠\" ◳◠▢◠▦ ◀◗◓ ◎▴◞◠▸◈◠▦ ◞◧▦◓◠ ▨◪▦◈◫◞◗▦◪ ◠◳◓▪◎▼◬▱◬▨ ◳◠▻▪▱◈▪◐▪ ◚◪ ◗◒◫▦◈◪▦ ▨◂◚◨▱◈▾◐▾ ◫◉◫▦ ◉◗◍▫▴ ◈◠◚◠ ◠◉◬▽◂◓▵'}\n",
      "{'dst': ' The kill a your are going to do something , with the <unk> . ', 'src': '◡◠▻◧▦ ◂▫◧◎◂◀◗▱ ◍◗◓◎◠◞◬ ◠▦▱◠◒◎◠◞▪▢ ◝◓▴▹◗▫ ◈◨◓▾◎▾▦◈◠ ◞▪▦◬◓◈◠ ◀◪▨▱▴◎◪ ◞◭◓◪◞◫▦◫▦ ◨▢◠◎◠◞▪▦▪▦ ◫▦◞◠▦▱◠◓▪▦ ◗◒◗▦◫ ▨◠▽◀◪▫◎◪◞◫▦◪ ▦▴◈◪▦ ◂▱◠◀◫▱▴▼▴◐◫▦◫ ◞◇◳▱◪◈◗▵'}\n",
      "{'dst': ' No , we were the know their a need to see the , then you were going on the in to stick in the hold of your ass ,', 'src': \"◝▾◀◀◠ ▰◠▫◞◂▦ ◚▴ ▰▴◀◀ ▮◫◎▻◞◂▦■ ◞◠◀◠▷ ◂◳▦◠▦◠▦ ◍◂◨◓◀◠▱▱ ◎◠◉▱◠◓▪▦▪▦ ▨◠▷◓◠◎◠▦▪ ▮▴◓◕◫◧ ◆◠◓▼◗◠'▽◬ ◚◪ ○▱▴▹ ▯◂◓▴▦ ◫▱▴ ▴◒▱▴◒▫◗◐◫ ◎◠◉▫◠ ▨◬◞◠ ◞▩◓◪◈◪ ◎◠◐▱▾▻ ▴▫▫◗▵\"}\n",
      "{'dst': \" I 'll never keep and I love you , I do n't know why I have\", 'src': '\"○◐▱◠◈◬◐▪▦▪ ◕◣◓◎◪▱◪◓◗▦◪ ◠◞▱◠ ◗▢◫▦ ◚▴◓◎▴■\" ◈◪◈◫ ◀◠▦◠▵'}\n",
      "{'dst': \" I should have been Yeah , for the <unk> 's while the last night that your eyes should be such a hot life to a\", 'src': '▭◠◀▴◓▴ ◕◇◓▴ ▫◠▦▪▨▱◠◓ ◧▱◠▽ ▽◪◓◫▦◈▴▦ ▨◠◉◠▦ ◀◗◓◈▴▦ ◍◠▢▱◠ ◒◭▻▷◪▱◫ ◧▱◈▾◐▾▦◨ ◞◇▽▱◪◈◗■ ◠▦▼◠▨ ▻◂▱◗◞ ◂▱◠◳◠ ▨◠◓◬◒◠▦ ▨◗◒◫ ◞◠▽◬◞▪▦▪ ◀▴▱◗◓▫◎▴◈◗▵'}\n",
      "{'dst': \" He 's in a time for the way of the <unk> from the the <unk> of <unk> , <unk> , and the from the think it was into the <unk>\", 'src': '◆▴◓◗▱◫◎■ ◄◠◈◓◫◈ ▫◠◓◠◍▪▦◈◠▦ ◳◠◞◠ ◈◬◒▪ ◗▱◠▦ ▴◈◫▱▴▦ ◠▦▼◠▨ ◠▽◓◬▱◬▨◉▪ ◢◠▫◠▱◠▦▱◠◓▼◠ ▨◨▫▱◠▦◠▦ 1 ◰▨◫◎ ◓▴◍▴◓◠▦◈▾◎▾▦◈◠▦ ◀◫◓ ◳◬▱ ◞◧▦◓◠ ◈◠ ◀◠◐◬◎◞◬▢▱▪▨ ◳◠▦▱◬◞◬ ◀◇▱◕◪◈◪ ◳▩▨◞▴▨ ▨◠▱◈▪▵'}\n",
      "{'dst': \" We 're going to Now , and the we 're so they 're going to get on the . \", 'src': '◝◂▱◗◚◳◠▱◬ ◈◫▻▱◧◎◠▫ ▤◧◀◪◓▫◂ ▬◠▱▢◠◈◗▱▱◠ ▨◧▦▾◳▱◠ ◫▱◕◗▱◗ ◧▱◠◓◠▨ \"◝◂▱◗◚▽◠ ◪▦◪◓▸◗■ ◀◗◓▱◗▨ ◚▴ ◀◪◓◠◀◪◓▱◫▨ ◓◨▷▾ ◚◪ ◞◭▨▾▦▴▫▱◪ ◈◧▱▾ ◂▱◠◓◠▨ ◚▴ ▴▱◀◪▫▫◪ ◂▱◨◎▱◨ ◀◗◓ ◀◠▨▪◒ ◠◉▪◞▪◳▱◠ ◞◂▦▾▼◨ ◀◪▨▱◗◳◧◓■\" ◫◍◠◈◪▱▴◓◗▦◫ ▨◨▱▱◠▦◈◬▵'}\n",
      "{'dst': ' I called your <unk> , the When <unk> is to buy some better and on the `` <unk> , here , <unk> is a family <unk> friend of , and you can get too many of my business', 'src': \"▮◠◗▦◞◀◨◓◳'◞ ◆▴▦▴▱ ◄◭◈◭◓▩ ◄◫▨▴ ▬◧◨▻▴■ ◞◠▫▪◒ ▦◧▨▫◠▱◠◓◬▦▪▦ ◈◠▷◠ ◉◧▨ ◕◬◈◠ ◈▪◒◬ ▷◗▢◎◪▫ ◞◨▦◠▦ ○▱◈◫ ◚◪ ▲◗◈▱ ◗▦◈◫◓◗◎▱◗ ◞◠▫◬◒ ◎◠◐◠▢◠▱◠◓▪◳▱◠ ◓◪▨◠◀◪▫ ▴▫◎▴▨ ◗◉◫▦ ◞◭▻◪◓◎◠◓▨◪▫ ▢◫▦▼◫◓◗▦◫▦ ◕◗◈◪◓◪▨ ◈◠▷◠ ◉◂▨ ◈◪▻◠◓▫◎◠▦ ◎◠◐◠▢◠▱◠◓▪▦◠ ◀▴▦▢◪▽◪▼▴◐◫▦◫ ◞◣◳▱▴◈◫▵\"}\n",
      "{'dst': \" This is against time , he 's a `` he 's not going to do it\", 'src': '◆◇◞▫▴◓◗ ◀◠◒▱◠◈▪ ◠▦▼◠▨ ◍◠◓▨▱◬ ◀◫◓ ◓◂▫◠ ◫▢▱◪◎◪▨ ▢◂◓◨▦◈◠ ▨◠▱◈◬▵'}\n"
     ]
    }
   ],
   "source": [
    "sld = make_submission_transformer(test_data[:10])\n",
    "print(*sld, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuksqPyVAmOu"
   },
   "outputs": [],
   "source": [
    "sld = make_submission_transformer(test_data)\n",
    "save_submission(sld, save_dir=submission_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFYpiYNzZfOW"
   },
   "source": [
    "### Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC5YQosWZc3y"
   },
   "outputs": [],
   "source": [
    "check_data = []\n",
    "with open(os.path.join(submission_dir, \"submission_20241116_123840.jsonl\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        temp_dict = json.loads(line)\n",
    "        check_data.append([temp_dict[\"src\"], temp_dict[\"dst\"]])\n",
    "check_data[500:600]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyONuoGFKN/xyUtoVk2CAMJu",
   "gpuType": "T4",
   "mount_file_id": "17l3jm-eqxnZ3al6e8DIROWKvEKpbkx8E",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03e572548b614b33a7e2867400e37400": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2766f4e8dd8b447d9d6ea0bbabc42c3b",
       "IPY_MODEL_719f768998624922b99c92af38071c63",
       "IPY_MODEL_e4bea96095194e8b8c91c59094be2be9"
      ],
      "layout": "IPY_MODEL_cb9b9306ead74461a98868ed0a3a166b"
     }
    },
    "0df9d2e9ec2940e6bf2a9e9de3578c3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2766f4e8dd8b447d9d6ea0bbabc42c3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0df9d2e9ec2940e6bf2a9e9de3578c3c",
      "placeholder": "​",
      "style": "IPY_MODEL_896097451c7f4e4088e4ce19267e4bf5",
      "value": " 63%"
     }
    },
    "540bd0c1c80f43fea52aac803a229441": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "719f768998624922b99c92af38071c63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0b8ccd6b934490dba720127948a4b9e",
      "max": 1172,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6606893463043eca4e5da2277d3259a",
      "value": 742
     }
    },
    "896097451c7f4e4088e4ce19267e4bf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "959f4b00b7fb4ff58b612332143e193c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0b8ccd6b934490dba720127948a4b9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb9b9306ead74461a98868ed0a3a166b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4bea96095194e8b8c91c59094be2be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_959f4b00b7fb4ff58b612332143e193c",
      "placeholder": "​",
      "style": "IPY_MODEL_540bd0c1c80f43fea52aac803a229441",
      "value": " 742/1172 [02:47&lt;01:46,  4.02it/s]"
     }
    },
    "f6606893463043eca4e5da2277d3259a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
